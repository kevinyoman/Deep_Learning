---
title: "ML Capstone"
author: "Kevin Nyoman Putra"
date: "2022-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(imager)
library(keras)
library(caret) 
```

# Business Question
For project capstone Machine Learning, we will choose data set “Where Were You”. In this project, we use unstructured data, pictures, and group into 3 category `Beach`, `Forest`, or `Mountain`. This is the best case to use Deep Learning Model for image classification.

# Read Data
Before read each images, we need to list folder's train for each category exists in our directory.
```{r}
folder_list <- list.files("train/")

folder_list
```
We combine with folder path as a preparation to access each folder, assigned in one variable.
```{r}
folder_path <- paste0("train/", folder_list, "/")

folder_path
```
Next, We use function `map()` to apply
```{r}
file_name <- map(folder_path, 
               function(x) paste0(x, list.files(x))
               ) %>% 
unlist()
```

Let's check how many image for data train.
```{r}
length(file_name)
```
# Data Wrangling
First, we need to check distribution of images dimension to properly input data's dimension for building a deep learning model. Scaling input will be processed later in Data Preprocessing.
```{r}
# Full Image Description
img <- load.image(file_name[1])

# img
dim(img)
```

Next, we will create custom function to store observations into dataframe. In this dataframe, we will have three columns, `height`, `width`, `filename`.
```{r}
# Function for acquiring width and height of an image
get_dim <- function(x){
  img <- load.image(x)

  df_img <- data.frame(height = height(img),
                       width = width(img),
                       filename = x
                       )

  return(df_img)
}

get_dim(file_name[1])
```

Let's use 1,000 sample:
```{r}
# # Randomly get 1000 sample images
#  set.seed(201)
#  sample_file <- sample(file_name, 1000)
# 
# # Run the get_dim() function for each image
#  file_dim <- map_df(sample_file, get_dim)
# 
#  head(file_dim, 10)
```
# EDA
Here the statistic for the image dimensions:
```{r}
# summary(file_dim)
```
Based on summary, data train have variance dimension:
- Maximum Width: 477
- Minimum Width: 100
- Maximum Height: 314
- Minimum Height: 100
We will scale width & height to 180, 300, respectfully. This is important when fitting our model, bigger dimension will get most of the information but there is caveat where it took longer to create model.

# Data Preprocessing / Cross Validation

## Data Augmentation
```{r}
# Desired height and width of images
target_size <- c(128, 128)

# Batch size for training the model
batch_size <- 128
```

##  Create setting for more variance image
To create more variance of our input images, we will try flip and rotate based on the setting

```{r}
# Image Generator
train_data_gen <- image_data_generator(rescale = 1/255, # Scaling pixel value
                                       horizontal_flip = T, # Flip image horizontally
                                       vertical_flip = T, # Flip image vertically 
                                       rotation_range = 45, # Rotate image from 0 to 45 degrees
                                       zoom_range = 0.25, # Zoom in or zoom out range
                                       validation_split = 0.2 # 20% data as validation data
                                       )
```

## Apply the seting into image generator
This section, we will split into training and validation dataset. In parameter color_mode, we will use "rgb" because our dataset has colors.
```{r}
# Training Dataset
train_image_array_gen <- flow_images_from_directory(directory = "train/", # Folder of the data
                                                    target_size = target_size, # target of the image dimension (160 x 260)  
                                                    color_mode = "rgb", # use RGB color
                                                    batch_size = batch_size , 
                                                    seed = 201,  # set random seed
                                                    subset = "training", # declare that this is for training data
                                                    generator = train_data_gen
                                                    )

# Validation Dataset
val_image_array_gen <- flow_images_from_directory(directory = "train/",
                                                  target_size = target_size, 
                                                  color_mode = "rgb", 
                                                  batch_size = batch_size ,
                                                  seed = 201,
                                                  subset = "validation", # declare that this is the validation data
                                                  generator = train_data_gen
                                                  )
```
## Check proportion of the imag class
```{r}
# Number of training samples
train_samples <- train_image_array_gen$n

# Number of validation samples
valid_samples <- val_image_array_gen$n

# Number of target classes/categories
output_n <- n_distinct(train_image_array_gen$classes)
```

Here proportion class of our datasets:
```{r}
# Get the class proportion
table("\nFrequency" = factor(train_image_array_gen$classes)
      ) %>% 
  prop.table()
```
Proportion class (beach, forest, mountain) of our datasets respectfuly 0.32, 0.30, 0.37. The differences not much, so we don't need to do balancing. Personally, we must balancing the class if the differences around 10 - 15%. If there is imbalance class, our model

# Build Model
```{r}
# input shape of the image
c(target_size, 3) 
```

```{r}
# Set Initial Random Weight
tensorflow::tf$random$set_seed(201)

model <- keras_model_sequential() %>% 
  
  # First convolutional layer
  layer_conv_2d(filters = 32,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = "same",
                activation = "relu",
                input_shape = c(target_size, 3)
                ) %>%  
  
  # Second convolutional layer
  layer_conv_2d(filters = 32,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = "same",
                activation = "relu", 
                ) %>% 
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  
  # Third convolutional layer
  layer_conv_2d(filters = 64,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = "same",
                activation = "relu", 
                ) %>%  
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  
  # Fifth convolutional layer
  layer_conv_2d(filters = 128,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = "same",
                activation = "relu", 
                ) %>%  
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  
  # Seventh convolutional layer
  layer_conv_2d(filters = 256,
                kernel_size = c(3,3), # 3 x 3 filters
                padding = "same",
                activation = "relu", 
                ) %>%   
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2,2)) %>%  
  
  # Flattening layer
  layer_flatten() %>% 
  
  # # Dense layer
  # layer_dense(units = 4096,
  #             activation = "relu") %>% 
  # 
  # # Dense layer
  # layer_dense(units = 4096,
  #             activation = "relu") %>%
  # 
  # Dense layer
  layer_dense(units = 1024,
              activation = "relu") %>%
  
  # Output layer
  layer_dense(name = "Output",
              units = 3, 
              activation = "softmax")

model
```

## save model for backup
```{r}
# model %>% 
  # save_model_tf(filepath = "model_capstone")
```

## Model Fitting
```{r}
model %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(lr = 0.001),
    metrics = "accuracy"
  )

## install pillow & SciPy in python environment

# Fit data into model
history <- model %>% 
  fit_generator(
  # training data
  train_image_array_gen,
  # training epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = 171, #51
  
  # validation data
  validation_data = val_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)

plot(history)
```
```{r eval=FALSE}
# save
model %>% 
  save_model_tf(filepath="model_cnn")
```

# Evaluate Model
```{r}
val_data <- data.frame(file_name = paste0("train/", val_image_array_gen$filenames)) %>% 
  mutate(class = str_extract(file_name, "beach|forest|mountain"))

tail(val_data, 10)
```
```{r}
# Function to convert image to array
image_prep <- function(x) {
  arrays <- lapply(x, function(path) {
    img <- image_load(path, target_size = target_size, 
                      grayscale = F # Set FALSE if image is RGB
                      )
    
    x <- image_to_array(img)
    x <- array_reshape(x, c(1, dim(x)))
    x <- x/255 # rescale image pixel
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}
```

```{r}
test_x <- image_prep(val_data$file_name)

# Check dimension of testing data set
dim(test_x)
```
## Prediction
```{r}
pred_test <- predict_classes(model, test_x) 

head(pred_test, 10)
```
```{r}
# Convert encoding to label
decode <- function(x){
  case_when(x == 0 ~ "beach",
            x == 1 ~ "forest",
            x == 2 ~ "mountain"
            )
}

pred_test <- sapply(pred_test, decode) 

head(pred_test, 10)
```

```{r}
confusionMatrix(as.factor(pred_test), 
                as.factor(val_data$class),
                mode = "everything"
                )
```
# Predict using test file

```{r}
folder_list_test <- list.files("test/")
filename_test <- paste0("test/", folder_list_test)

test_x_real <- image_prep(filename_test)
pred_test_real <- predict_classes(model, test_x_real) 

pred_test_real <- sapply(pred_test_real, decode) 
```


## assign to submission

```{r}
submit_il <- read.csv("image-data-test.csv")
submit_il$label <- pred_test_real

write.csv(submit_il,"submission-kevin.csv", row.names = FALSE)  
```
